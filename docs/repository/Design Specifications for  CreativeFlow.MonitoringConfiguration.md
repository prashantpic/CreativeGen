# Software Design Specification: CreativeFlow.MonitoringConfiguration

## 1. Introduction

### 1.1 Purpose
This document specifies the design for the `CreativeFlow.MonitoringConfiguration` repository. This repository is responsible for defining and managing the configurations for the entire observability stack of the CreativeFlow AI platform. This includes metrics collection (Prometheus), alerting (Alertmanager), visualization (Grafana), and log aggregation (Loki and Promtail). The configurations herein ensure that the platform's health, performance, operational status, and business KPIs are comprehensively tracked, visualized, and alerted upon.

### 1.2 Scope
The scope of this repository includes:
*   Prometheus server configuration (`prometheus.yml`), including global settings, service discovery, scrape configurations for various targets, recording rules, and alerting rules.
*   Alertmanager configuration (`alertmanager.yml`), including alert routing, receivers (email, Slack, PagerDuty/Opsgenie), inhibition rules, and notification templates.
*   Grafana provisioning configurations for datasources (Prometheus, Loki) and dashboards.
*   Grafana dashboard definitions (JSON) for system overview, application performance, infrastructure health, AI model monitoring, business process monitoring, and log analysis.
*   Loki configuration (`loki-config.yaml`) for centralized log storage, indexing, and retention.
*   Promtail configurations (`promtail-config.yaml` and specific scrape configs) for log shipping from various sources, including Kubernetes pods and custom AI models.

This repository contains configuration files (YAML, JSON, PromQL, LogQL) and does not produce executable code itself. These configurations are intended to be deployed to and consumed by the respective monitoring tools.

### 1.3 Definitions and Acronyms
*   **APM**: Application Performance Monitoring
*   **CI/CD**: Continuous Integration / Continuous Deployment
*   **CDN**: Content Delivery Network
*   **DCGM**: NVIDIA Data Center GPU Manager
*   **DR**: Disaster Recovery
*   **ELK**: Elasticsearch, Logstash, Kibana
*   **IaC**: Infrastructure as Code
*   **KPI**: Key Performance Indicator
*   **Loki**: Log aggregation system by Grafana Labs
*   **NFR**: Non-Functional Requirement
*   **PromQL**: Prometheus Query Language
*   **LogQL**: Loki Query Language
*   **PWA**: Progressive Web Application
*   **RPO**: Recovery Point Objective
*   **RTO**: Recovery Time Objective
*   **SAST**: Static Application Security Testing
*   **SDS**: Software Design Specification
*   **SLO**: Service Level Objective
*   **SRE**: Site Reliability Engineering
*   **UAT**: User Acceptance Testing
*   **WAF**: Web Application Firewall
*   **WCAG**: Web Content Accessibility Guidelines
*   **YAML**: YAML Ain't Markup Language

## 2. System Overview
The `CreativeFlow.MonitoringConfiguration` repository acts as the "source of truth" for the configuration of the CreativeFlow AI platform's observability stack. It centralizes all configurations, enabling version control, automated deployment, and consistency across different environments. This repository ensures that all components of the platform, from infrastructure to applications and AI models, are adequately monitored, and that operational teams receive timely alerts for any issues. The configurations are designed to support proactive monitoring, detailed troubleshooting, performance analysis, and business KPI tracking.

The primary tools configured by this repository are:
*   **Prometheus**: For time-series metrics collection and alerting.
*   **Alertmanager**: For handling alerts generated by Prometheus and routing them to appropriate notification channels.
*   **Grafana**: For visualizing metrics and logs via dashboards.
*   **Loki**: For aggregating logs from all system components.
*   **Promtail**: For shipping logs to Loki.

## 3. Design Considerations

### 3.1 Configuration as Code
All monitoring configurations will be managed as code, stored in this Git repository. This allows for versioning, peer reviews, and automated deployment via CI/CD pipelines, ensuring consistency and auditability.

### 3.2 Modularity and Reusability
Configurations will be structured modularly. For instance, Prometheus scrape configurations and alert rules will be in separate files, grouped by service or function, promoting clarity and reusability. Grafana dashboards will be defined as JSON and provisioned, allowing for templatization where appropriate.

### 3.3 Scalability
The monitoring stack configurations must be designed to scale with the CreativeFlow AI platform. This includes using appropriate service discovery mechanisms for Prometheus (e.g., Kubernetes SD), designing efficient PromQL queries, and ensuring the logging pipeline (Promtail -> Loki) can handle the expected log volume.

### 3.4 Security
*   Sensitive information within configurations (e.g., API keys for Alertmanager receivers like PagerDuty or Slack webhooks) must be managed through a secrets management system (e.g., HashiCorp Vault) and injected at deployment time, not stored in plain text in this repository. Placeholders will be used in the configuration files.
*   Access to monitoring systems (Grafana, Prometheus UI, etc.) will be secured via authentication and authorization.

### 3.5 Maintainability
Configurations will be well-documented with comments explaining the purpose of rules, scrape jobs, and dashboard panels. A consistent naming convention for metrics, alerts, and dashboards will be adopted.

### 3.6 Technology Stack
*   **Prometheus**: v2.51.1 or later
*   **Alertmanager**: v0.27.0 or later
*   **Grafana**: v10.4.2 or later
*   **Loki**: v2.9.6 or later
*   **Promtail**: v2.9.6 or later
*   **Languages**: YAML, JSON, PromQL, LogQL

## 4. Detailed Design
This section details the structure and content of the configuration files managed within this repository, as defined in the `file_structure_json`.

### 4.1 Prometheus Configuration (`prometheus/`)

#### 4.1.1 `prometheus/prometheus.yml`
*   **Purpose**: Main Prometheus server configuration.
*   **Requirement(s) Addressed**: DEP-005, QA-003
*   **Structure & Key Elements**:
    *   `global`:
        *   `scrape_interval`: e.g., `15s` (default for most jobs)
        *   `evaluation_interval`: e.g., `15s`
        *   `external_labels`: e.g., `monitor: 'creativeflow-main'`
    *   `rule_files`: Array of paths to rule files.
        *   `- /etc/prometheus/rules/recording_rules.yml`
        *   `- /etc/prometheus/rules/application_alerts.yml`
        *   `- /etc/prometheus/rules/infrastructure_alerts.yml`
        *   `- /etc/prometheus/rules/ai_model_alerts.yml`
        *   `- /etc/prometheus/rules/business_process_alerts.yml`
    *   `scrape_config_files`: Array of paths to scrape configuration files.
        *   `- /etc/prometheus/scrape_configs/node_exporter_targets.yml`
        *   `- /etc/prometheus/scrape_configs/application_service_targets.yml`
        *   `- /etc/prometheus/scrape_configs/database_targets.yml`
        *   `- /etc/prometheus/scrape_configs/message_queue_targets.yml`
        *   `- /etc/prometheus/scrape_configs/kubernetes_targets.yml`
        *   `- /etc/prometheus/scrape_configs/gpu_targets.yml`
        *   `- /etc/prometheus/scrape_configs/custom_ai_model_targets.yml`
    *   `alerting`:
        *   `alertmanagers`:
            *   `static_configs`:
                *   `targets`: [`<ALERTMANAGER_URL_1>:<ALERTMANAGER_PORT>`, `<ALERTMANAGER_URL_2>:<ALERTMANAGER_PORT>`] (e.g., `alertmanager:9093`)
*   **Logic**: Defines intervals, loads all rule and scrape configuration files, and configures communication with Alertmanager instances.

#### 4.1.2 Prometheus Rules (`prometheus/rules/`)

##### 4.1.2.1 `prometheus/rules/recording_rules.yml`
*   **Purpose**: Define Prometheus recording rules for pre-computation.
*   **Requirement(s) Addressed**: DEP-005, QA-003
*   **Structure & Key Elements**:
    yaml
    groups:
      - name: example_recording_rules
        interval: 30s # Optional, overrides global evaluation_interval
        rules:
          - record: job:http_requests_total:sum_rate5m
            expr: sum(rate(http_requests_total[5m])) by (job)
            labels:
              quantile: "0.99" # Example label
          # Add more recording rules for:
          # - API error rates per endpoint
          # - Service-level latencies (avg, p90, p95, p99)
          # - Resource utilization aggregates (e.g., cluster-wide CPU/memory usage)
          # - Queue processing rates
    
*   **Logic**: Contains rules using PromQL `expr` to compute new time series specified by `record`.

##### 4.1.2.2 `prometheus/rules/application_alerts.yml`
*   **Purpose**: Define Prometheus alerting rules for core application services.
*   **Requirement(s) Addressed**: DEP-005, QA-003, QA-003.1
*   **Structure & Key Elements**:
    yaml
    groups:
      - name: application_service_alerts
        rules:
          - alert: HighErrorRateAPIGateway
            expr: sum(rate(http_requests_total{job="api-gateway", code=~"5.."}[5m])) / sum(rate(http_requests_total{job="api-gateway"}[5m])) > 0.05
            for: 5m
            labels:
              severity: critical
              service: api-gateway
            annotations:
              summary: "High API Gateway Error Rate (Instance {{ $labels.instance }})"
              description: "API Gateway is experiencing an error rate greater than 5% for the last 5 minutes."
              runbook_url: "<RUNBOOK_URL_API_GATEWAY_ERRORS>"
          # Add alerts for:
          # - Odoo backend high latency/error rates
          # - n8n workflow failures/stuck jobs
          # - User authentication service failures
          # - Subscription service errors
          # - Collaboration service high latency/errors
          # - Notification service queue backup/failures
          # - Web frontend high client-side error rates (if metrics exposed)
    
*   **Logic**: Contains rules using PromQL `expr` to detect issues. `for` defines duration for alert to fire. `labels` and `annotations` provide context.

##### 4.1.2.3 `prometheus/rules/infrastructure_alerts.yml`
*   **Purpose**: Define alerting rules for infrastructure components.
*   **Requirement(s) Addressed**: DEP-005, QA-003, QA-003.1
*   **Structure & Key Elements**:
    yaml
    groups:
      - name: infrastructure_component_alerts
        rules:
          - alert: HostHighCpuLoad
            expr: node_load15{job="node_exporter"} / count without(cpu)(node_cpu_seconds_total{job="node_exporter",mode="system"}) > 0.8
            for: 10m
            labels:
              severity: warning
              component: server
            annotations:
              summary: "High CPU Load on {{ $labels.instance }}"
              description: "CPU load (15 min avg) is above 80% on host {{ $labels.instance }}."
              runbook_url: "<RUNBOOK_URL_HIGH_CPU>"
          - alert: HostLowDiskSpace
            expr: (node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.lxcfs", mountpoint!="/boot"} / node_filesystem_size_bytes{fstype!~"tmpfs|fuse.lxcfs", mountpoint!="/boot"}) * 100 < 10
            for: 5m
            labels:
              severity: critical
              component: server
            annotations:
              summary: "Low Disk Space on {{ $labels.instance }} (mountpoint {{ $labels.mountpoint }})"
              description: "Less than 10% disk space available on {{ $labels.mountpoint }} for host {{ $labels.instance }}."
              runbook_url: "<RUNBOOK_URL_LOW_DISK>"
          # Add alerts for:
          # - PostgreSQL: high replication lag, low connections available, slow queries
          # - Redis: high memory usage, instance down
          # - RabbitMQ: high queue depth, unacknowledged messages, cluster partition
          # - Kubernetes: NodeNotReady, PodCrashLooping, high API server latency
          # - MinIO: disk health, replication issues
    
*   **Logic**: Similar to application alerts, but focused on metrics from infrastructure exporters.

##### 4.1.2.4 `prometheus/rules/ai_model_alerts.yml`
*   **Purpose**: Define alerting rules for custom AI models.
*   **Requirement(s) Addressed**: DEP-005, QA-003, QA-003.1, INT-007
*   **Structure & Key Elements**:
    yaml
    groups:
      - name: ai_model_alerts
        rules:
          - alert: CustomAIModelHighInferenceLatency
            expr: histogram_quantile(0.99, sum(rate(custom_model_inference_latency_seconds_bucket{job="custom-ai-models", model_name="{{ $labels.model_name }}"}[5m])) by (le, model_name)) > <MODEL_SPECIFIC_LATENCY_THRESHOLD_SECONDS>
            for: 5m
            labels:
              severity: warning
              service: custom-ai-models
              model_name: "{{ $labels.model_name }}"
            annotations:
              summary: "High P99 Inference Latency for AI Model {{ $labels.model_name }}"
              description: "P99 inference latency for model {{ $labels.model_name }} is above threshold."
              runbook_url: "<RUNBOOK_URL_AI_MODEL_LATENCY>"
          - alert: CustomAIModelHighErrorRate
            expr: sum(rate(custom_model_inference_errors_total{job="custom-ai-models", model_name="{{ $labels.model_name }}"}[5m])) / sum(rate(custom_model_inference_requests_total{job="custom-ai-models", model_name="{{ $labels.model_name }}"}[5m])) > <MODEL_SPECIFIC_ERROR_RATE_THRESHOLD>
            for: 10m
            labels:
              severity: critical
              service: custom-ai-models
              model_name: "{{ $labels.model_name }}"
            annotations:
              summary: "High Error Rate for AI Model {{ $labels.model_name }}"
              description: "Error rate for model {{ $labels.model_name }} is above threshold."
              runbook_url: "<RUNBOOK_URL_AI_MODEL_ERRORS>"
          # Add alerts for:
          # - GPU resource saturation per model (utilization, memory)
          # - Model drift indicators (if metrics are defined and exported)
          # - Low throughput for a specific model
    
*   **Logic**: Alerts tailored to metrics specific to custom AI model deployments. Thresholds may vary per model.

##### 4.1.2.5 `prometheus/rules/business_process_alerts.yml`
*   **Purpose**: Define alerting rules based on key business process metrics.
*   **Requirement(s) Addressed**: DEP-005, QA-003, QA-003.1
*   **Structure & Key Elements**:
    yaml
    groups:
      - name: business_process_alerts
        rules:
          - alert: LowRegistrationSuccessRate
            expr: sum(rate(user_registration_success_total[1h])) / sum(rate(user_registration_attempts_total[1h])) < 0.95
            for: 30m
            labels:
              severity: critical
              business_process: user_registration
            annotations:
              summary: "Low User Registration Success Rate"
              description: "User registration success rate has fallen below 95% in the last hour."
              runbook_url: "<RUNBOOK_URL_REGISTRATION_ISSUES>"
          - alert: HighPaymentFailureRate
            expr: sum(rate(payment_failure_total[1h])) / sum(rate(payment_attempts_total[1h])) > 0.10
            for: 1h
            labels:
              severity: critical
              business_process: payment_processing
            annotations:
              summary: "High Payment Failure Rate"
              description: "Payment failure rate is above 10% in the last hour."
              runbook_url: "<RUNBOOK_URL_PAYMENT_ISSUES>"
          - alert: AIGenerationSuccessRateLow
            # Assuming a metric like `ai_generation_completed_total{status="success"}` and `ai_generation_attempted_total`
            expr: sum(rate(ai_generation_completed_total{status="success"}[5m])) / sum(rate(ai_generation_attempted_total[5m])) < 0.98
            for: 15m
            labels:
              severity: warning
              business_process: ai_generation
              kpi_id: "KPI-004"
            annotations:
              summary: "AI Generation Success Rate Below Target"
              description: "AI generation success rate is below 98% (KPI-004)."
              runbook_url: "<RUNBOOK_URL_AI_GENERATION_FAILURES>"
          # Add alerts for:
          # - Unusual credit consumption patterns
          # - High API error rates for key API users
          # - Low feature adoption rates for new features (based on custom event metrics)
    
*   **Logic**: Alerts based on aggregated business metrics or specific KPIs.

#### 4.1.3 Prometheus Scrape Configurations (`prometheus/scrape_configs/`)

##### 4.1.3.1 `prometheus/scrape_configs/node_exporter_targets.yml`
*   **Purpose**: Scrape system-level metrics from all servers.
*   **Requirement(s) Addressed**: DEP-005, QA-003
*   **Structure & Key Elements**:
    yaml
    # This file should be referenced by scrape_config_files in prometheus.yml
    - job_name: 'node_exporter'
      # Option 1: Static config (if IPs are fixed and few)
      # static_configs:
      #   - targets: ['<SERVER_IP_1>:9100', '<SERVER_IP_2>:9100']
      # Option 2: File-based service discovery (managed by Ansible or other CM tool)
      file_sd_configs:
        - files:
            - '/etc/prometheus/file_sd/node_exporter_targets.json' # or .yml
          refresh_interval: 5m
      # Option 3: Kubernetes service discovery if node_exporter is a DaemonSet
      # kubernetes_sd_configs:
      #   - role: endpoints
      #     namespaces:
      #       names:
      #         - kube-system # Or wherever node_exporter service is
      # relabel_configs:
      #   - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name]
      #     action: keep
      #     regex: node-exporter # Or the actual service name/label
      #   - source_labels: [__address__]
      #     target_label: __address__
      #     regex: ([^:]+)(:[0-9]+)?
      #     replacement: ${1}:9100 # Ensure correct port
      #   - source_labels: [__meta_kubernetes_pod_node_name]
      #     target_label: instance
    
*   **Logic**: Defines how Prometheus discovers and scrapes `node_exporter` instances. File-based SD is common for self-hosted, Ansible-managed fleets. Kubernetes SD if `node_exporter` is deployed as a DaemonSet.

##### 4.1.3.2 `prometheus/scrape_configs/application_service_targets.yml`
*   **Purpose**: Scrape custom metrics from application services.
*   **Requirement(s) Addressed**: DEP-005, QA-003
*   **Structure & Key Elements**:
    yaml
    # API Gateway (Nginx/Kong/KrakenD or custom)
    - job_name: 'api-gateway'
      # Assuming Kubernetes SD if applicable, or static if a few fixed instances
      kubernetes_sd_configs:
        - role: pod # Or 'service' if there's a dedicated service object
          namespaces:
            names: ['<API_GATEWAY_NAMESPACE>']
      relabel_configs:
        - source_labels: [__meta_kubernetes_pod_label_app] # Adjust label as needed
          action: keep
          regex: 'api-gateway-app' # Example label value
        - source_labels: [__meta_kubernetes_pod_container_port_name]
          action: keep
          regex: 'metrics' # If port is named 'metrics'
        # ... other relabeling for instance, job
    # Odoo Backend
    - job_name: 'odoo-backend'
      # Assuming Odoo exposes metrics via JMX exporter or custom /metrics endpoint
      # Example for custom /metrics endpoint via Kubernetes SD
      kubernetes_sd_configs:
        - role: pod
          namespaces:
            names: ['<ODOO_NAMESPACE>']
      relabel_configs:
        - source_labels: [__meta_kubernetes_pod_label_app]
          action: keep
          regex: 'odoo-app'
        - source_labels: [__meta_kubernetes_pod_container_port_name]
          action: keep
          regex: 'metrics-odoo'
    # n8n Workflows
    - job_name: 'n8n-workflows'
      kubernetes_sd_configs:
        - role: pod
          namespaces:
            names: ['<N8N_NAMESPACE>']
      relabel_configs:
        - source_labels: [__meta_kubernetes_pod_label_app]
          action: keep
          regex: 'n8n-app'
        - source_labels: [__meta_kubernetes_pod_container_port_name]
          action: keep
          regex: 'metrics-n8n'
    # Add other application services: Auth, UserManagement, CreativeManagement, etc.
    # Each might have its own job_name and discovery mechanism.
    
*   **Logic**: Defines scrape jobs for each major application component that exposes Prometheus metrics. Kubernetes SD is heavily used if services are containerized.

##### 4.1.3.3 `prometheus/scrape_configs/database_targets.yml`
*   **Purpose**: Scrape metrics from database exporters.
*   **Requirement(s) Addressed**: DEP-005, QA-003
*   **Structure & Key Elements**:
    yaml
    - job_name: 'postgres_exporter'
      static_configs:
        - targets: ['<POSTGRES_EXPORTER_HOST_1>:<POSTGRES_EXPORTER_PORT>', '<POSTGRES_EXPORTER_HOST_2>:<POSTGRES_EXPORTER_PORT>'] # For HA setup
          labels:
            env: '<ENVIRONMENT_NAME>' # e.g., production, staging
    - job_name: 'redis_exporter'
      static_configs:
        - targets: ['<REDIS_EXPORTER_HOST_1>:<REDIS_EXPORTER_PORT>', '<REDIS_EXPORTER_HOST_2>:<REDIS_EXPORTER_PORT>'] # For HA setup
          labels:
            env: '<ENVIRONMENT_NAME>'
    
*   **Logic**: Targets for `postgres_exporter` and `redis_exporter`. Typically these exporters run alongside or near the database instances. Static configs are common here if the exporter locations are fixed.

##### 4.1.3.4 `prometheus/scrape_configs/message_queue_targets.yml`
*   **Purpose**: Scrape metrics from RabbitMQ exporter.
*   **Requirement(s) Addressed**: DEP-005, QA-003
*   **Structure & Key Elements**:
    yaml
    - job_name: 'rabbitmq_exporter'
      static_configs:
        - targets: ['<RABBITMQ_EXPORTER_HOST_1>:<RABBITMQ_EXPORTER_PORT>', '<RABBITMQ_EXPORTER_HOST_2>:<RABBITMQ_EXPORTER_PORT>']
          labels:
            env: '<ENVIRONMENT_NAME>'
    
*   **Logic**: Targets the RabbitMQ exporter instances.

##### 4.1.3.5 `prometheus/scrape_configs/kubernetes_targets.yml`
*   **Purpose**: Scrape metrics from Kubernetes components.
*   **Requirement(s) Addressed**: DEP-005, QA-003
*   **Structure & Key Elements**:
    yaml
    # Kubelet (cAdvisor metrics for containers/pods)
    - job_name: 'kubernetes-cadvisor'
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      kubernetes_sd_configs:
        - role: node
      relabel_configs:
        # ... (Standard cAdvisor relabeling, often provided by Prometheus operator or Helm charts)
    # Kube-state-metrics
    - job_name: 'kubernetes-kube-state-metrics'
      kubernetes_sd_configs:
        - role: endpoints
          namespaces:
            names: [kube-system] # Or where kube-state-metrics is deployed
      relabel_configs:
        - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name]
          action: keep
          regex: kube-state-metrics
        # ... (More relabeling)
    # API Server
    - job_name: 'kubernetes-apiservers'
      kubernetes_sd_configs:
        - role: endpoints
          namespaces:
            names: [default] # Or wherever kubernetes service is
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
        - source_labels: [__meta_kubernetes_service_name]
          action: keep
          regex: kubernetes
        # ... (More relabeling)
    
*   **Logic**: Defines jobs for scraping internal Kubernetes metrics from kubelet (cAdvisor), kube-state-metrics, and the API server. Requires appropriate RBAC for Prometheus to access these endpoints.

##### 4.1.3.6 `prometheus/scrape_configs/gpu_targets.yml`
*   **Purpose**: Scrape GPU metrics using NVIDIA DCGM exporter.
*   **Requirement(s) Addressed**: DEP-005, QA-003, INT-007
*   **Structure & Key Elements**:
    yaml
    - job_name: 'dcgm_exporter'
      # Assuming DCGM exporter is deployed as a DaemonSet on GPU nodes
      kubernetes_sd_configs:
        - role: pod
          namespaces:
            names: ['<GPU_OPERATOR_NAMESPACE>'] # Or where dcgm-exporter pods are
      relabel_configs:
        - source_labels: [__meta_kubernetes_pod_label_app] # Use appropriate label
          action: keep
          regex: 'dcgm-exporter'
        - source_labels: [__meta_kubernetes_pod_node_name]
          target_label: instance
        - source_labels: [__address__]
          target_label: __address__
          regex: ([^:]+)(:[0-9]+)?
          replacement: ${1}:9400 # Default DCGM exporter port
      metric_relabel_configs:
        # Add any necessary metric relabeling here
    
*   **Logic**: Targets `dcgm-exporter` instances, typically running as a DaemonSet on GPU-enabled Kubernetes nodes.

##### 4.1.3.7 `prometheus/scrape_configs/custom_ai_model_targets.yml`
*   **Purpose**: Scrape metrics from custom AI models.
*   **Requirement(s) Addressed**: DEP-005, QA-003, INT-007
*   **Structure & Key Elements**:
    yaml
    - job_name: 'custom-ai-models'
      kubernetes_sd_configs:
        - role: pod # Or service if models are exposed via K8s services
          namespaces:
            names: ['<AI_MODELS_NAMESPACE>'] # Namespace where custom models are deployed
      relabel_configs:
        - source_labels: [__meta_kubernetes_pod_label_ai_model_name] # Example label on model pods
          action: replace
          target_label: model_name
        - source_labels: [__meta_kubernetes_pod_label_ai_model_version] # Example label
          action: replace
          target_label: model_version
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true # Scrape if annotation prometheus.io/scrape="true"
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+) # Use path from annotation
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        # Add more relabeling as needed
    
*   **Logic**: Uses Kubernetes service discovery to find custom AI model pods/services that expose metrics. Assumes pods are labeled with `ai_model_name` and `ai_model_version` and use standard Prometheus scrape annotations (`prometheus.io/scrape`, `prometheus.io/path`, `prometheus.io/port`).

### 4.2 Alertmanager Configuration (`alertmanager/`)

#### 4.2.1 `alertmanager/alertmanager.yml`
*   **Purpose**: Main Alertmanager configuration.
*   **Requirement(s) Addressed**: DEP-005, QA-003.1
*   **Structure & Key Elements**:
    yaml
    global:
      resolve_timeout: 5m
      # SMTP settings for email notifications
      smtp_smarthost: '<SMTP_HOST>:<SMTP_PORT>'
      smtp_from: '<ALERTMANAGER_FROM_EMAIL>'
      smtp_auth_username: '<SMTP_USERNAME>'
      smtp_auth_password: '<SMTP_PASSWORD_SECRET_PLACEHOLDER>' # Manage via secrets
      # Slack settings
      slack_api_url: '<SLACK_WEBHOOK_URL_SECRET_PLACEHOLDER>' # Manage via secrets
    
    route:
      group_by: ['alertname', 'service', 'severity']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 4h # How often to resend alerts if still firing
      receiver: 'default-receiver' # Default receiver if no specific route matches
    
      routes:
        - receiver: 'critical-pagerduty'
          match_re:
            severity: 'critical|P1'
          continue: true # Allow other routes to match as well if needed
    
        - receiver: 'warning-slack'
          match_re:
            severity: 'warning|P2|P3'
          continue: true
    
        - receiver: 'info-email'
          match_re:
            severity: 'info|P4'
    
        # Route for specific services
        - receiver: 'db-team-pagerduty'
          match:
            service: 'postgresql'
            severity: 'critical'
    
        - receiver: 'ai-infra-slack'
          match:
            service: 'custom-ai-models'
            severity: 'warning|critical'
    
    receivers:
      - name: 'default-receiver'
        email_configs:
          - to: '<DEFAULT_OPS_EMAIL_GROUP>'
            send_resolved: true
    
      - name: 'critical-pagerduty'
        pagerduty_configs:
          - service_key: '<PAGERDUTY_SERVICE_KEY_SECRET_PLACEHOLDER>' # Manage via secrets
            send_resolved: true
            description: "{{ range .Alerts }}{{ .Annotations.summary }}\n{{ end }}"
    
      - name: 'warning-slack'
        slack_configs:
          - channel: '#alerts-warning'
            send_resolved: true
            title: '[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }} for {{ .CommonLabels.service }}'
            text: "{{ range .Alerts }}*Summary:* {{ .Annotations.summary }}\n*Description:* {{ .Annotations.description }}\n*Runbook:* {{ .Annotations.runbook_url }}\n{{ end }}"
    
      - name: 'info-email'
        email_configs:
          - to: '<INFO_EMAIL_DISTRIBUTION_LIST>'
            send_resolved: true
    
      - name: 'db-team-pagerduty'
        pagerduty_configs:
          - service_key: '<DB_TEAM_PAGERDUTY_KEY_SECRET_PLACEHOLDER>'
            send_resolved: true
    
      - name: 'ai-infra-slack'
        slack_configs:
          - channel: '#alerts-ai-infra'
            send_resolved: true
            text: "{{ range .Alerts }}<!channel> AI Model Alert: {{ .Annotations.summary }} for {{ .Labels.model_name }}. Details: {{ .Annotations.description }}\nRunbook: {{ .Annotations.runbook_url }}\n{{ end }}"
    
    inhibit_rules:
      - source_match:
          severity: 'critical'
        target_match:
          severity: 'warning'
        # Don't send warning if a critical alert with the same labels is already firing.
        equal: ['alertname', 'service', 'instance']
    
    templates:
      - '/etc/alertmanager/templates/*.tmpl'
    
*   **Logic**: Defines how alerts are grouped, routed to different receivers based on labels (severity, service), and how notifications are formatted. Inhibition rules prevent alert storms.

#### 4.2.2 `alertmanager/templates/default_notification.tmpl`
*   **Purpose**: Default Go template for alert notifications.
*   **Requirement(s) Addressed**: DEP-005, QA-003.1
*   **Structure & Key Elements (Illustrative)**:
    go
    {{ define "email.default.subject" }}[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }} - {{ .CommonLabels.service | default "general" }}{{ end }}

    {{ define "email.default.html" }}
    <!DOCTYPE html><html><body>
    <h1>{{ .Status | toUpper }} Alert: {{ .CommonLabels.alertname }}</h1>
    {{ range .Alerts }}
        <h2>{{ .Annotations.summary }}</h2>
        <p><strong>Severity:</strong> {{ .Labels.severity }}</p>
        <p><strong>Service:</strong> {{ .Labels.service | default "N/A" }}</p>
        <p><strong>Instance:</strong> {{ .Labels.instance | default "N/A" }}</p>
        <p><strong>Description:</strong> {{ .Annotations.description }}</p>
        {{ if .Annotations.runbook_url }}<p><strong>Runbook:</strong> <a href="{{ .Annotations.runbook_url }}">{{ .Annotations.runbook_url }}</a></p>{{ end }}
        <p><strong>Starts at:</strong> {{ .StartsAt.Format "2006-01-02 15:04:05 MST" }}</p>
        <hr/>
    {{ end }}
    <p>Grouped by: {{ .GroupLabels.String }}</p>
    </body></html>
    {{ end }}

    {{ define "slack.default.text" }}
    {{ range .Alerts -}}
    *[{{ .Status | toUpper }}]* Alert: *{{ .Annotations.summary }}*
    > Severity: `{{ .Labels.severity }}`
    > Service: `{{ .Labels.service | default "N/A" }}`
    > Description: {{ .Annotations.description }}
    {{ if .Annotations.runbook_url }}> Runbook: <{{ .Annotations.runbook_url }}|Link> {{ end }}
    {{ end -}}
    {{ end }}
    
*   **Logic**: Uses Go templating to customize the appearance of email and Slack notifications. Similar templates can be created for other receivers.

### 4.3 Grafana Configuration (`grafana/`)

#### 4.3.1 Datasource Provisioning (`grafana/provisioning/datasources/`)

##### 4.3.1.1 `grafana/provisioning/datasources/prometheus_ds.yml`
*   **Purpose**: Provision Prometheus datasource in Grafana.
*   **Requirement(s) Addressed**: DEP-005, QA-003
*   **Structure & Key Elements**:
    yaml
    apiVersion: 1
    datasources:
      - name: Prometheus-Main
        type: prometheus
        url: http://<PROMETHEUS_SERVICE_HOST>:<PROMETHEUS_PORT> # e.g., http://prometheus:9090
        access: server # Or proxy
        isDefault: true
        jsonData:
          timeInterval: "15s"
        editable: false
    
*   **Logic**: Defines the connection parameters for Grafana to access the Prometheus server.

##### 4.3.1.2 `grafana/provisioning/datasources/loki_ds.yml`
*   **Purpose**: Provision Loki datasource in Grafana.
*   **Requirement(s) Addressed**: DEP-005, QA-003
*   **Structure & Key Elements**:
    yaml
    apiVersion: 1
    datasources:
      - name: Loki-Main
        type: loki
        url: http://<LOKI_SERVICE_HOST>:<LOKI_PORT> # e.g., http://loki:3100
        access: server
        jsonData:
          maxLines: 1000
        editable: false
    
*   **Logic**: Defines connection parameters for Grafana to access the Loki server.

#### 4.3.2 Dashboard Provisioning (`grafana/provisioning/dashboards/`)

##### 4.3.2.1 `grafana/provisioning/dashboards/dashboard_provider.yml`
*   **Purpose**: Configure Grafana dashboard provisioning.
*   **Requirement(s) Addressed**: DEP-005, QA-003
*   **Structure & Key Elements**:
    yaml
    apiVersion: 1
    providers:
      - name: 'default-dashboards'
        orgId: 1
        folder: 'CreativeFlow AI Dashboards'
        type: file
        disableDeletion: false
        editable: true # Allows UI edits but IaC will overwrite on restart if allowUiUpdates is false
        options:
          path: /etc/grafana/provisioning/dashboards/json # Path inside Grafana container
          # For production, consider allowUiUpdates: false to enforce IaC
        allowUiUpdates: true # Set to false to manage strictly via JSON files
    
*   **Logic**: Tells Grafana to load dashboard JSON files from the specified directory.

#### 4.3.3 Grafana Dashboards (`grafana/dashboards/*.json`)
These files are JSON objects representing Grafana dashboards. They define panels, queries (PromQL for metrics, LogQL for logs), variables, and layout.

##### 4.3.3.1 `grafana/dashboards/system_overview_dashboard.json`
*   **Purpose**: High-level system overview.
*   **Requirement(s) Addressed**: DEP-005, QA-003
*   **Key Panels (Conceptual)**:
    *   Overall API Gateway request rate & error percentage.
    *   Odoo backend health status (CPU, memory, key job queues).
    *   n8n workflow execution success/failure rates.
    *   AI Processing Cluster overall GPU utilization and job queue.
    *   PostgreSQL DB health (connections, replication lag, slow queries).
    *   RabbitMQ overview (queue depths, message rates).
    *   Redis health (memory usage, hit rate).
    *   Number of active user sessions.
*   **Templating**: Variable for `environment` (prod, staging).

##### 4.3.3.2 `grafana/dashboards/custom_ai_model_monitoring.json`
*   **Purpose**: Monitor custom AI models.
*   **Requirement(s) Addressed**: DEP-005, QA-003, INT-007
*   **Key Panels (Conceptual)**:
    *   Per-model inference latency (P50, P90, P99).
    *   Per-model request throughput.
    *   Per-model error rate.
    *   Per-model GPU utilization, GPU memory usage.
    *   Per-model CPU/memory usage of serving pods.
*   **Templating**: Variables for `model_name`, `model_version`, `environment`.

##### 4.3.3.3 `grafana/dashboards/logs_ai_model_dashboard.json`
*   **Purpose**: Visualize logs from custom AI models.
*   **Requirement(s) Addressed**: DEP-005, QA-003, INT-007
*   **Key Panels (Conceptual)**:
    *   Log stream panel (using Loki datasource).
    *   Log counts by level (INFO, WARN, ERROR).
    *   Top errors by message.
    *   Histogram of log occurrences over time.
*   **Templating**: Variables for `model_name`, `model_version`, `kubernetes_pod_name`, `log_level`, `environment`. Queries use LogQL.

*Self-documented*: Other dashboards would be created for Kubernetes cluster details, PostgreSQL deep-dive, RabbitMQ deep-dive, Node Exporter (server health), Application-specific dashboards (e.g., Odoo performance, API gateway details). Each would use relevant PromQL queries.

### 4.4 Loki/Promtail Configuration (`loki/`, `promtail/`)

#### 4.4.1 `loki/loki-config.yaml`
*   **Purpose**: Configure Loki server.
*   **Requirement(s) Addressed**: DEP-005, QA-003
*   **Structure & Key Elements**:
    yaml
    auth_enabled: false # Or true with appropriate auth config if needed

    server:
      http_listen_port: 3100
      grpc_listen_port: 9096
    
    ingester:
      lifecycler:
        address: 127.0.0.1
        ring:
          kvstore:
            store: inmemory
          replication_factor: 1
      chunk_idle_period: 5m       # Any chunk not receiving new logs in this time will be flushed
      chunk_target_size: 1572864  # Target size of chunks (1.5MB)
      max_chunk_age: 1h           # Chunks are flushed if they reach this age
      chunk_retain_period: 1m     # How long to keep chunks in memory after flushing
    
    schema_config:
      configs:
        - from: 2020-10-24 # Date of schema change or initial setup
          store: boltdb-shipper # Or other stores like cassandra, bigtable, dynamodb
          object_store: s3 # Using S3-compatible storage (MinIO)
          schema: v11 # Or latest compatible schema version
          index:
            prefix: index_
            period: 24h # How often to create new index tables/shards
    
    storage_config:
      boltdb_shipper: # If using boltdb-shipper with S3
        active_index_directory: /loki/boltdb-shipper-active
        cache_location: /loki/boltdb-shipper-cache
        cache_ttl: 24h
        shared_store: s3
      aws: # This section is for S3-compatible storage like MinIO
        s3: http://<MINIO_USER>:<MINIO_PASSWORD_PLACEHOLDER>@<MINIO_HOST>:<MINIO_PORT>/<LOKI_BUCKET_NAME>
        s3forcepathstyle: true # Required for MinIO usually
        # region: <YOUR_MINIO_REGION_IF_APPLICABLE> - usually not needed for self-hosted MinIO
    
    compactor:
      working_directory: /loki/compactor
      compaction_interval: 10m
      retention_enabled: true
      retention_delete_delay: 2h
      # Retention period is configured via table_manager for boltdb-shipper
    
    limits_config:
      # Define query limits, ingestion rate limits, etc.
      # For example:
      # max_query_length: 721h
      # max_query_parallelism: 32
      # ingestion_rate_mb: 100
      # ingestion_burst_size_mb: 200
      # Per-tenant limits can also be configured if multi-tenancy is used
      retention_period: 30d # Overall default retention if not overridden by table_manager
    
    table_manager: # For boltdb-shipper to manage retention per table/period
      retention_deletes_enabled: true
      retention_period: 30d # Example: Keep logs for 30 days in active storage
                           # Longer term archival could be managed by MinIO lifecycle policies
                           # or by configuring separate Loki instances/storage for archive tiers.
    
*   **Logic**: Configures Loki's server behavior, storage backend (MinIO via S3 API), indexing schema, and log retention policies. Specific retention: operational debug logs (14 days hot, 30 days warm - this needs more nuanced config, Loki usually has one retention period for active data, archival is separate), application info/audit logs (90 days hot, 1 year warm/cold - this would be a tiered approach possibly with MinIO lifecycle rules or separate Loki for archive), security audit logs (min 12 months active - requires setting retention_period to at least 365d for security logs). The example above shows a general 30d retention, which would need adjustment or more complex setup for varied retention.

#### 4.4.2 `promtail/promtail-config.yaml`
*   **Purpose**: Configure Promtail log shipping agent.
*   **Requirement(s) Addressed**: DEP-005, QA-003
*   **Structure & Key Elements**:
    yaml
    server:
      http_listen_port: 9080
      grpc_listen_port: 0 # Typically not used by Promtail itself for listening
    
    positions:
      filename: /tmp/positions.yaml # Path to store read offsets
    
    clients:
      - url: http://<LOKI_SERVICE_HOST>:<LOKI_PORT>/loki/api/v1/push # e.g., http://loki:3100/loki/api/v1/push
        # batchwait: 1s
        # batchsize: 1048576 # 1MB
        # tenant_id: '<TENANT_ID_IF_MULTI_TENANT_LOKI>'
    
    scrape_configs:
      # Include scrape configs from separate files
      # This structure assumes promtail/scrape_configs/*.yaml are individual scrape job definitions.
      # However, promtail typically has all scrape_configs listed directly here.
      # For better modularity, you might manage snippets and cat them or use a config management tool.
      # For this SDS, we will assume snippets are included conceptually.
      # Actual Promtail config might list jobs directly or use an include mechanism if supported by deployment.
    
      # Example: System logs (can be a snippet from another file)
      - job_name: system
        static_configs:
          - targets:
              - localhost
            labels:
              job: varlogs
              __path__: /var/log/*log # Collect system logs
        # pipeline_stages: # Optional, to parse specific system log formats
    
      # Placeholder to indicate inclusion of Kubernetes and AI model logs
      # The actual content would come from kubernetes_logs.yaml and ai_model_specific_logs.yaml
      # For a real promtail.yml, you'd likely embed the content of those files here.
      # ---- Start of kubernetes_logs.yaml content ----
      - job_name: kubernetes-pods
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels:
              - __meta_kubernetes_pod_node_name
            target_label: __host__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - action: replace
            source_labels:
              - __meta_kubernetes_namespace
            target_label: namespace
          - action: replace
            source_labels:
              - __meta_kubernetes_pod_name
            target_label: pod
          - action: replace
            source_labels:
              - __meta_kubernetes_pod_container_name
            target_label: container
          - replacement: /var/log/pods/*$1/*.log # Path to container logs
            source_labels:
              - __meta_kubernetes_pod_uid
              - __meta_kubernetes_pod_container_name
            target_label: __path__
        pipeline_stages:
          - cri: {} # For CRI-compatible runtimes like Docker, containerd
          # Attempt to parse as JSON, add labels from JSON fields
          - json:
              expressions:
                level: level
                message: message
                timestamp: time
                traceID: traceID # If logs contain traceID
                spanID: spanID   # If logs contain spanID
          - labels:
              level:
              traceID:
              spanID:
          - timestamp:
              source: timestamp # Use 'timestamp' field from JSON if available
              format: RFC3339Nano
              fallback_formats: # Add other common timestamp formats if needed
                - RFC3339
          - output:
              source: message # Use 'message' field as the log line content
      # ---- End of kubernetes_logs.yaml content ----
    
      # ---- Start of ai_model_specific_logs.yaml content ----
      - job_name: ai-custom-models # Specific job for AI models if needed for different parsing
        kubernetes_sd_configs:
          - role: pod
            selectors: # Example: only select pods with this label
              - role: LabeledPods
                label: "app.kubernetes.io/component=ai-model"
        relabel_configs:
          # Similar to kubernetes-pods, but can add specific model_name, model_version labels
          - source_labels: [__meta_kubernetes_pod_label_ai_model_name]
            target_label: model_name
          - source_labels: [__meta_kubernetes_pod_label_ai_model_version]
            target_label: model_version
          # ... other common K8s relabeling ...
          - replacement: /var/log/pods/*$1/*.log
            source_labels:
              - __meta_kubernetes_pod_uid
              - __meta_kubernetes_pod_container_name
            target_label: __path__
        pipeline_stages:
          - cri: {}
          # Specific parsing for AI model logs if they have a unique structure
          - json:
              expressions:
                model_event: event_type
                inference_id: id
                # ... other model-specific fields ...
          - labels:
              model_event:
              inference_id:
          # ... timestamp and output stages ...
      # ---- End of ai_model_specific_logs.yaml content ----
    
*   **Logic**: Defines Loki server endpoint, positions file, and scrape_configs for discovering and processing log files/streams. `pipeline_stages` allow for parsing (e.g., JSON, regex) and labeling logs.

##### 4.4.2.1 `promtail/scrape_configs/kubernetes_logs.yaml` (Conceptual content, usually part of main `promtail-config.yaml`)
*   **Purpose**: Configure Promtail for Kubernetes pod log collection.
*   **Requirement(s) Addressed**: DEP-005, QA-003, INT-007 (as custom models run in K8s)
*   **Structure & Key Elements (as would be embedded in `promtail-config.yaml` `scrape_configs`):**
    yaml
    # This content is typically part of the main promtail-config.yaml
    # - job_name: kubernetes-pods-stdout # For stdout/stderr of containers
    #   kubernetes_sd_configs:
    #     - role: pod
    #   pipeline_stages:
    #     - docker: {} # or cri: {} depending on K8s container runtime
    #   relabel_configs:
    #     - source_labels: ['__meta_kubernetes_pod_node_name']
    #       target_label: '__host__'
    #     - action: labelmap
    #       regex: __meta_kubernetes_pod_label_(.+)
    #     - action: replace
    #       source_labels: ['__meta_kubernetes_namespace']
    #       target_label: 'namespace'
    #     - action: replace
    #       source_labels: ['__meta_kubernetes_pod_name']
    #       target_label: 'pod'
    #     - action: replace
    #       source_labels: ['__meta_kubernetes_pod_container_name']
    #       target_label: 'container'
    #     # Add more relabeling to extract useful metadata
    
*   **Logic**: Uses Kubernetes Service Discovery (`kubernetes_sd_configs`) to find pods. `relabel_configs` extract Kubernetes metadata (namespace, pod name, container name, labels) and apply them as Loki labels. `pipeline_stages` with `docker: {}` or `cri: {}` handle standard container log formats. JSON parsing can be added if container logs are structured.

##### 4.4.2.2 `promtail/scrape_configs/ai_model_specific_logs.yaml` (Conceptual content)
*   **Purpose**: Specific log collection for custom AI models.
*   **Requirement(s) Addressed**: DEP-005, QA-003, INT-007
*   **Structure & Key Elements (as would be embedded in `promtail-config.yaml` `scrape_configs`):**
    yaml
    # This content is typically part of the main promtail-config.yaml
    # - job_name: custom-ai-model-logs
    #   kubernetes_sd_configs:
    #     - role: pod
    #       # Use selectors to target pods with specific AI model labels
    #       selectors:
    #         - role: LabeledPods # Arbitrary role, use actual K8s selectors
    #           label: "creativeflow.ai/model-type=custom"
    #   pipeline_stages:
    #     - docker: {} # or cri: {}
    #     # Potentially add custom parsing stages if AI models have unique log formats
    #     # - regex:
    #     #     expression: "^(?P<timestamp>\\S+) (?P<level>\\S+) (?P<component>\\S+): (?P<message>.*)$"
    #     # - labels:
    #     #     level:
    #     #     component:
    #   relabel_configs:
    #     # Similar to kubernetes_logs.yaml but could add specific model_name/version labels
    #     - source_labels: ['__meta_kubernetes_pod_label_model_name']
    #       target_label: 'model_name'
    #     - source_labels: ['__meta_kubernetes_pod_label_model_version']
    #       target_label: 'model_version'
    #     # ... other standard K8s relabeling
    
*   **Logic**: Targets specific Kubernetes pods running custom AI models (e.g., via labels). May include custom `pipeline_stages` for parsing model-specific log formats.

## 5. Data Management
This repository primarily manages configuration data in YAML and JSON formats. There is no direct application data (like user data or generated assets) stored or managed by these configurations. Data management aspects relevant to the monitoring tools themselves include:
*   **Prometheus**: Time-series database (TSDB) storage configuration (retention, block duration) is part of `prometheus.yml` implicitly or via command-line flags to the Prometheus server. These are managed by the deployment scripts for Prometheus (outside this repo's scope, but this repo's configs assume Prometheus is running).
*   **Loki**: Log storage configuration (backend, retention) is defined in `loki/loki-config.yaml`.
*   **Grafana**: Dashboard and datasource configurations are stored as JSON/YAML here and provisioned. Grafana itself uses a database (e.g., SQLite, PostgreSQL) for its internal state, which is outside this repo's scope.

## 6. Deployment Considerations
*   **CI/CD Integration**: All configurations in this repository must be version-controlled in Git. Changes should trigger a CI/CD pipeline that validates the configurations (e.g., `promtool check rules`, `promtool check config`, `alertmanager --config.file ... --check.config`, Grafana JSON schema validation, YAML linting).
*   **Deployment to Servers**:
    *   Validated configurations should be deployed to the respective monitoring servers (Prometheus, Alertmanager, Grafana, Loki, Promtail instances) using configuration management tools like Ansible or by updating Kubernetes ConfigMaps/Secrets if the monitoring stack is K8s-native.
    *   Prometheus, Alertmanager, Grafana, and Loki servers need to be reloaded or restarted to apply new configurations. Promtail typically reloads its config dynamically or on restart.
*   **Secrets Management**: Placeholders like `<SMTP_PASSWORD_PLACEHOLDER>`, `<SLACK_WEBHOOK_URL_SECRET_PLACEHOLDER>`, `<PAGERDUTY_SERVICE_KEY_SECRET_PLACEHOLDER>`, `<MINIO_PASSWORD_PLACEHOLDER>` must be replaced with actual secrets during deployment, ideally sourced from a secure vault (e.g., HashiCorp Vault) by the deployment automation.
*   **Environment Specificity**: Configurations may need to be slightly different per environment (dev, staging, prod). This can be managed using branching strategies in Git, environment-specific variable files with Ansible, or Helm chart value overrides if deploying to Kubernetes.
*   **Service Discovery**: Prometheus and Promtail rely heavily on service discovery. Ensure that the chosen discovery mechanisms (Kubernetes SD, file SD, etc.) are correctly configured and that target services/exporters are discoverable.

## 7. Non-Functional Requirements Addressed
*   **QA-003 (Proactive production monitoring and observability)**: The entire suite of configurations (Prometheus, Grafana, Loki/Promtail) is designed to provide comprehensive monitoring.
*   **QA-003.1 (Alerting Thresholds and Procedures)**: Addressed by `alertmanager.yml` and the Prometheus alert rule files (`*.alerts.yml`), which define SMART thresholds and will be integrated with escalation procedures.
*   **DEP-005 (Comprehensive operational monitoring, logging, and maintenance setup)**: This repository defines the "monitoring and logging" aspects of DEP-005. Maintenance procedures for the monitoring tools themselves would be part of operational runbooks.
*   **INT-007 (Monitoring and Observability of custom models)**: Specifically addressed by:
    *   `prometheus/rules/ai_model_alerts.yml`
    *   `prometheus/scrape_configs/custom_ai_model_targets.yml`
    *   `prometheus/scrape_configs/gpu_targets.yml`
    *   `grafana/dashboards/custom_ai_model_monitoring.json`
    *   `grafana/dashboards/logs_ai_model_dashboard.json`
    *   `promtail/scrape_configs/ai_model_specific_logs.yaml` (conceptual content for Promtail)

This SDS provides a blueprint for creating the configuration files that will drive the CreativeFlow AI platform's observability. The specific PromQL and LogQL queries, alert thresholds, and dashboard panel details will require further refinement based on deployed application behavior and operational experience.